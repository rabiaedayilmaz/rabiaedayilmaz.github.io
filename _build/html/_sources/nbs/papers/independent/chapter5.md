# Knowledge Distillation

## Be Your Own Teacher: Improve the Performance of Convolutional NeuralNetworks via Self Distillation

::::{grid}
:gutter: 1

:::{grid-item-card} To Read
[Paper](https://arxiv.org/pdf/1905.08094.pdf)
:::
Here, self-distillation that is a general training framework is presented. It enhances accuracy/performances of CNN by shrinking its size. Different from traditional knowledge distillation, it distills knowledge within network itself. The knowledge in the deeper layers of the network squeezed into shallow ones. Furthermore, it provides flexibility of depth-wise scalable inference on resource-limited devices.

[Github](https://github.com/luanyunteng/pytorch-be-your-own-teacher)

---

