
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1. Image Generation - Breast Cancer - Papers &#8212; rey&#39;s log</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2. rey’s Art" href="../art/art.html" />
    <link rel="prev" title="Welcome to my blog" href="../../intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logom.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">rey's log</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to my blog
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PAPERS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Image Generation - Breast Cancer - Papers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ART
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../art/art.html">
   2. rey’s Art
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/rabiaedayilmaz/rabiaedayilmaz.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/rabiaedayilmaz/rabiaedayilmaz.github.io/issues/new?title=Issue%20on%20page%20%2Fnbs/papers/image-generation-breast-cancer.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/nbs/papers/image-generation-breast-cancer.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bci-breast-cancer-immunohistochemical-image-generation-through-pyramid-pix2pix">
   1.1. BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#breaking-the-dilemma-of-medical-image-to-image-translation">
   1.2. Breaking the Dilemma of Medical Image-to-image Translation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagnostic-strategies-for-breast-cancer-detection-from-image-generation-to-classification-strategies-using-artificial-intelligence-algorithms">
   1.3. Diagnostic Strategies for Breast Cancer Detection: From Image Generation to Classification Strategies Using Artificial Intelligence Algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mammograpgy">
     1.3.1. Mammograpgy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ultrasound">
     1.3.2. Ultrasound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#magnetic-resonance-imaging-mri">
     1.3.3. Magnetic Resonance Imaging (MRI)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ea-gans-edge-aware-generative-adversarial-networks-for-crossmodality-mr-image-synthesis">
   1.4. Ea-GANs: Edge-Aware Generative Adversarial Networks for CrossModality MR Image Synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-adversarial-networks-an-overview">
   1.5. Generative Adversarial Networks An Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fully-connected-gans">
     1.5.1. Fully Connected GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-gans">
     1.5.2. Convolutional GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-gans">
     1.5.3. Conditional GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gans-with-inference-models">
     1.5.4. GANs with Inference Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adversarial-autoencoders">
     1.5.5. Adversarial Autoencoders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-to-image-translation-with-conditional-adversarial-networks">
   1.6. Image-to-Image Translation with Conditional Adversarial Networks
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Image Generation - Breast Cancer - Papers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bci-breast-cancer-immunohistochemical-image-generation-through-pyramid-pix2pix">
   1.1. BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#breaking-the-dilemma-of-medical-image-to-image-translation">
   1.2. Breaking the Dilemma of Medical Image-to-image Translation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagnostic-strategies-for-breast-cancer-detection-from-image-generation-to-classification-strategies-using-artificial-intelligence-algorithms">
   1.3. Diagnostic Strategies for Breast Cancer Detection: From Image Generation to Classification Strategies Using Artificial Intelligence Algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mammograpgy">
     1.3.1. Mammograpgy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ultrasound">
     1.3.2. Ultrasound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#magnetic-resonance-imaging-mri">
     1.3.3. Magnetic Resonance Imaging (MRI)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ea-gans-edge-aware-generative-adversarial-networks-for-crossmodality-mr-image-synthesis">
   1.4. Ea-GANs: Edge-Aware Generative Adversarial Networks for CrossModality MR Image Synthesis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-adversarial-networks-an-overview">
   1.5. Generative Adversarial Networks An Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fully-connected-gans">
     1.5.1. Fully Connected GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-gans">
     1.5.2. Convolutional GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-gans">
     1.5.3. Conditional GANs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gans-with-inference-models">
     1.5.4. GANs with Inference Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adversarial-autoencoders">
     1.5.5. Adversarial Autoencoders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-to-image-translation-with-conditional-adversarial-networks">
   1.6. Image-to-Image Translation with Conditional Adversarial Networks
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="image-generation-breast-cancer-papers">
<h1><span class="section-number">1. </span>Image Generation - Breast Cancer - Papers<a class="headerlink" href="#image-generation-breast-cancer-papers" title="Permalink to this headline">#</a></h1>
<section id="bci-breast-cancer-immunohistochemical-image-generation-through-pyramid-pix2pix">
<h2><span class="section-number">1.1. </span>BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix<a class="headerlink" href="#bci-breast-cancer-immunohistochemical-image-generation-through-pyramid-pix2pix" title="Permalink to this headline">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://arxiv.org/abs/2204.11425">Paper</a></p>
</div>
</div>
</div>
<figure class="align-default" id="directive-fig">
<img alt="../../_images/bci-1.png" src="../../_images/bci-1.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.1 </span><span class="caption-text">Samples from BCI Dataset</span><a class="headerlink" href="#directive-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Human epidermal growth factor receptor 2 (HER2) is important for formulating a precise treatment of breast cancer. Evaluation of HER is performed with immunohistochemical techniques (IHC), however IHC is very costly to perform. Thus, in this paper, a breast cancer immunohistochemical (BCI) benchmark is proposed for the first time. The goal is to synthesize IHC data from the paired hematoxylin and eosin (HE) stained images. BCI dataset contains 4870 paired images with different expression levels (0, 1+, 2+, and 3+). Furthermore, a pyramid pix2pix universal image translation method is used. This paper, for the first time investigates this problem and tries to solve it.</p>
<p>Breast cancer is a common type in woman and leading cause of death. Accurate diagnosis and treatment are key factors to survival. Histopathological checking is a gold standard to identify cancer. It is done by staining tumor materials and getting hematoxylin and eosin (HE) slices that later will be observed by pathologists through the microscope or analyzing the digitized whole slice images (WSI). After diagnosis, preparing precise treatment is an essential step. For this step, expression of specific proteins are checked, such as HER2. Over expression of HER2 indicates tendency to aggressive clinical behaviour. However, to conduct evaluation of HER2 is really expensive. Therefore, it is  aimed to create HER2 images from IHC-stained slices.</p>
<p>IHC 0: no stain, IHC 1+: barely perceptible, IHC 2+: weak to moderate complete staining, and IHC 3+: complete and intense staining.</p>
<figure class="align-default" id="id1">
<img alt="../../_images/bci-2.png" src="../../_images/bci-2.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.2 </span><span class="caption-text">Examples of slices and HER2 expressions</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id2">
<img alt="../../_images/bci-3.png" src="../../_images/bci-3.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.3 </span><span class="caption-text">How the Dataset is Formed</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The data scanning equipment is Hamamatsu NanoZommer S60. For each pathological tissue sample, the doctor will cut two tissue from it, one for HE staining and the other one for HER2 detection. Thus, there will be differences between two tissue samples and furthermore, samples will be stretched or squeezed during slice preparation. To align both images, registration process is followed and projection transformation that is done by mapping squares of images and moreover, elastix registraion is applied. After these steps, post-processing is applied to remove black border between blocks and fill it with surrounding content. Lastly, the blank or not-well aligned ares are filtered out.</p>
<figure class="align-default" id="id3">
<img alt="../../_images/bci-4.png" src="../../_images/bci-4.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.4 </span><span class="caption-text">The Model: pix2pix</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The L1 loss is directly calculates the difference between ground truth and generated image. Multi-scale loss is formed for scale transformation that applies low-pass filter to smooth the image and down-smapling smoothed image.</p>
<figure class="align-default" id="id4">
<img alt="../../_images/bci-5.png" src="../../_images/bci-5.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.5 </span><span class="caption-text">The Overall Objective Function</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The method used in this paper is outperformed.</p>
<figure class="align-default" id="id5">
<img alt="../../_images/bci-6.png" src="../../_images/bci-6.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.6 </span><span class="caption-text">Benchmark Results</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id6">
<img alt="../../_images/bci-7.png" src="../../_images/bci-7.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.7 </span><span class="caption-text">HER2 Visualizations</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The accuracy of the outcomes of the model is evaluated by pathologists and achieved 37.5% and 40.0% accuracy performance. Briefly, this is a challenging task and there is a need for more effective model.</p>
<hr class="docutils" />
</div>
</div>
</section>
<section id="breaking-the-dilemma-of-medical-image-to-image-translation">
<h2><span class="section-number">1.2. </span>Breaking the Dilemma of Medical Image-to-image Translation<a class="headerlink" href="#breaking-the-dilemma-of-medical-image-to-image-translation" title="Permalink to this headline">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://arxiv.org/abs/2110.06465">Paper</a></p>
</div>
</div>
</div>
<p>Supervised Pix2Pix and unsupervised Cycle-consistency are two models dominates the field of medical image-to-image translation. But both of them are not ideal. Moreover, it requires paired and well-pixel aligned images that makes it really challengable especially in medical field and not always feasible due to respiratory motion or anatomical changes between times of acquired paired images. Cycle-consistency works well on unpaired or misaligned images. However, accuracy performance is not optimal and may produce multiple solutions. To break this dilemma, in this paper, RegGAN is proposed for medical image-to-image translation. It is based on theory of “loss-correction”. Misaligned target images are considered as noisy labels and generator is trained with an additional registration network. The main goal is to search for a common solution both for image-to-image translation and registration tasks. In this paper, it is demonstrated that RegGAN can be easily combined with these models and improve their performance. The key outcome of this paper is that they demonstrated using registrations improves significantly the performance of image-to-iamge translation because of adaptively eliminating the noise.</p>
<figure class="align-default" id="id7">
<img alt="../../_images/reggan-1.png" src="../../_images/reggan-1.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.8 </span><span class="caption-text">Comparison of Models</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id8">
<img alt="../../_images/reggan-2.png" src="../../_images/reggan-2.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.9 </span><span class="caption-text">Correction Loss</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id9">
<img alt="../../_images/reggan-3.png" src="../../_images/reggan-3.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.10 </span><span class="caption-text">Comparison of Results</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
<section id="diagnostic-strategies-for-breast-cancer-detection-from-image-generation-to-classification-strategies-using-artificial-intelligence-algorithms">
<h2><span class="section-number">1.3. </span>Diagnostic Strategies for Breast Cancer Detection: From Image Generation to Classification Strategies Using Artificial Intelligence Algorithms<a class="headerlink" href="#diagnostic-strategies-for-breast-cancer-detection-from-image-generation-to-classification-strategies-using-artificial-intelligence-algorithms" title="Permalink to this headline">#</a></h2>
<section id="mammograpgy">
<h3><span class="section-number">1.3.1. </span>Mammograpgy<a class="headerlink" href="#mammograpgy" title="Permalink to this headline">#</a></h3>
<p>It is used to screen breast tissue to detect abnormalities indicate cancer or another related diseases. It is recommend since it has 85% sensibility. Mammography uses low-doses of X-ray to form a picture of the internal breast tissue. To achieve this, breast is compressed by two platelets to mitigate the dispersion of the rays and obtain better picture without using high-doses of X-ray. Specialists look for the different zones like shape, size, contrast, edges, bright spots. The most common symptoms are calcifications and masses. Recently, Breast Tomosynthesis (BT) that allows 3D reconstruction and Contrast-Enhanced Mammography (CEM) that improves image resolution by injecting a contrast agent have been proposed.</p>
</section>
<section id="ultrasound">
<h3><span class="section-number">1.3.2. </span>Ultrasound<a class="headerlink" href="#ultrasound" title="Permalink to this headline">#</a></h3>
<p>It is non-invasive and non-irradiating technique and useswaves to create images from breast. In order to create images, a transducer sends high-frequency sound waves (&gt;=20kHz) and measures the reflected ones. The image is constructed by reflected wave sound from the internal tissues. Ultrasound has three purposes: i) assessing and determining the abnormality condition like solid or fluid-filled, ii) as an auxiliary scree tool when patient has dense breasts and mammography is not reliable enough, and iii) a guide to develop a biopsy in the suspected abnormality.
To analyze ultrasound images several computer-aided diagnose (CAD) systems are proposed and their common objective is to improve resolution of the image. Another proposed method is micro-bubbles that are injected into the abnormalities detected at first sight.
Elastography is the technique to measure the tumor displacement when compressedusing a spatial transducer.</p>
</section>
<section id="magnetic-resonance-imaging-mri">
<h3><span class="section-number">1.3.3. </span>Magnetic Resonance Imaging (MRI)<a class="headerlink" href="#magnetic-resonance-imaging-mri" title="Permalink to this headline">#</a></h3>
<p>Breast MRI (BMRI) uses a magnetic field and radio waves to create a detailed image. Generally, 1.5T magnet with a contrast (usually gadolinium) is used. When the magnet is turned on, the magnetic field temporarily realigns the water molecules, so when radio waves are applied the emitted radiation is captured using specific-designed coils that are located at breast positions. These coils transform the captured radiation into electrical signals. The main goal is to get images of breast symmetry and the possible changes in the parenchymal tissue (reflection of the proportion of glandular tissue to fatty tissue). One of the problems of BMRI is that false-positive (specifity) rate, since this technique can detect low-size masses that are benign. To mitigate tihs issue, nanomaterials have been developed to stick to the cancer masses but not the benign ones as well as contrast agents.</p>
<p>There are other approaches such as microwave radiation, CT, PET etc.</p>
<p>Additionally, there is a recent image generation technique: Infrared Thermography (IRT). Temperature is an indicator of health. In breast cancer, when tumor exists it makes use of nutrients for its growth (angiogenesis) that results in increase of metabolism thus the temperature around the tumor.</p>
</section>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9322973/">Paper</a></p>
</div>
</div>
</div>
<p>Breast cancer is the leading death of women worldwide and according to World Health Organization (WHO) approximately 16% of diagnosed as malignant is the reason of that. Thus, early stage detection is important to have highest chance for survival. Breast cancer develops when any lump begins an angiogenesis process that causes the development of new blood vessels and capillaries from the existent vasculature.
Its mortality rate is 69% in emergent countries. In emergent countries, late diagnosis increases this rate.</p>
<p>There are several technologies used to obtain breast tissue:</p>
</div>
</div>
</section>
<section id="ea-gans-edge-aware-generative-adversarial-networks-for-crossmodality-mr-image-synthesis">
<h2><span class="section-number">1.4. </span>Ea-GANs: Edge-Aware Generative Adversarial Networks for CrossModality MR Image Synthesis<a class="headerlink" href="#ea-gans-edge-aware-generative-adversarial-networks-for-crossmodality-mr-image-synthesis" title="Permalink to this headline">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://ieeexplore.ieee.org/document/8629301">Paper</a></p>
</div>
</div>
</div>
<p>Medical image synthesis is important topic. It maps from given source-modality image to unknown target-modality. There is wide range area of usage: virtual dataset creation, missing image imputation, image super-resolution etc. Currently there are two approaches: atlas-based methods that calculates atlas-to-image transformation in paired images (mostly healthy patient atlas data is available) and learning-based methods.</p>
<p>Magnetic resonance imaging (MRI) is widely used protocol and each MR modality (T1-w, T2-w, FLAIR etc.) reveals unique visual characteristics. To benefit from complementary information from multiple imaging modalities, <em>cross-modality MR synthesis</em> has gained attention. But most existing methods onyl focus on minimizing pixel/voxel-wise intensity difference but ignore textural details of the image content structure that affects the quality of synthesized image. So, in this paper a cross-modality MR image synthesis method is proposed. It is edge-aware generative adversarial network (EA-GAN). There, the edge information that represents textural structure and depicts the boundaries of different objects is integrated. Two learning strategies are proposed: gEA-GAN (generator-induced) and dEA-GAN (discriminator-induced). gEA-GAN integrates the edge information via its generator and dEA-GAN does same via both generator and discriminator, so that edge similarity is also learned adversarialy. Proposed EA-GANs are 3D based and utilize hierarchical features to capture contextual information. dEA-GAN outperforms and SOTA method for cross-modality MR image synthesis (07/2019) and it is generalizable.</p>
<p>The edge maps are computed by using Sobel operator since it is simple and derivative can easily be computed.</p>
<figure class="align-default" id="id10">
<img alt="../../_images/eagan-1.png" src="../../_images/eagan-1.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.11 </span><span class="caption-text">Sobel Filter and Edges</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id11">
<img alt="../../_images/eagan-2.png" src="../../_images/eagan-2.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.12 </span><span class="caption-text">Objective of Generator in gEA-GAN</span><a class="headerlink" href="#id11" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id12">
<img alt="../../_images/eagan-3.png" src="../../_images/eagan-3.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.13 </span><span class="caption-text">Objective of Discriminator in gEA-GAN</span><a class="headerlink" href="#id12" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id13">
<img alt="../../_images/eagan-4.png" src="../../_images/eagan-4.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.14 </span><span class="caption-text">Final Objective Function of gEA-GAN</span><a class="headerlink" href="#id13" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id14">
<img alt="../../_images/eagan-5.png" src="../../_images/eagan-5.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.15 </span><span class="caption-text">Objective of Generator in dEA-GAN</span><a class="headerlink" href="#id14" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id15">
<img alt="../../_images/eagan-6.png" src="../../_images/eagan-6.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.16 </span><span class="caption-text">Objective of Discriminator in dEA-GAN</span><a class="headerlink" href="#id15" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Similarly, the final objective function is summation of generator and discriminator objectives.
Architecture consists of three modules: generator, discriminator and edge detector.</p>
<figure class="align-default" id="id16">
<img alt="../../_images/eagan-7.png" src="../../_images/eagan-7.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.17 </span><span class="caption-text">Architecture of EA-GANs</span><a class="headerlink" href="#id16" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id17">
<img alt="../../_images/eagan-8.png" src="../../_images/eagan-8.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.18 </span><span class="caption-text">Architecture fo Generator</span><a class="headerlink" href="#id17" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id18">
<img alt="../../_images/eagan-10.png" src="../../_images/eagan-10.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.19 </span><span class="caption-text">Results on BRATS2015 Dataset</span><a class="headerlink" href="#id18" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id19">
<img alt="../../_images/eagan-11.png" src="../../_images/eagan-11.png" />
<figcaption>
<p><span class="caption-number">Fig. 1.20 </span><span class="caption-text">Comparison with Pix2Pix on Different 2D Datasets</span><a class="headerlink" href="#id19" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
<section id="generative-adversarial-networks-an-overview">
<h2><span class="section-number">1.5. </span>Generative Adversarial Networks An Overview<a class="headerlink" href="#generative-adversarial-networks-an-overview" title="Permalink to this headline">#</a></h2>
<section id="fully-connected-gans">
<h3><span class="section-number">1.5.1. </span>Fully Connected GANs<a class="headerlink" href="#fully-connected-gans" title="Permalink to this headline">#</a></h3>
<p>The first GAN architecture that used fully connected neural networks for both generator and discriminator.</p>
</section>
<section id="convolutional-gans">
<h3><span class="section-number">1.5.2. </span>Convolutional GANs<a class="headerlink" href="#convolutional-gans" title="Permalink to this headline">#</a></h3>
<p>DCGANs (deep convolutional gans) are dominant in this group. They make use of strided and fractionally stridede convolutions that allows spatial downsampling and upsampling.</p>
</section>
<section id="conditional-gans">
<h3><span class="section-number">1.5.3. </span>Conditional GANs<a class="headerlink" href="#conditional-gans" title="Permalink to this headline">#</a></h3>
<p>The conditional setting is both generator and discriminator networks are class-conditional. They have more advantage for multimodal data generation.</p>
</section>
<section id="gans-with-inference-models">
<h3><span class="section-number">1.5.4. </span>GANs with Inference Models<a class="headerlink" href="#gans-with-inference-models" title="Permalink to this headline">#</a></h3>
<p>The generator consists of two networks: the encoder(inference network) and the decoder. Both of them jointly trained to fool discriminator. The discriminator receives pairs of (x, z) vectors has to determine which pair constitute genuine tuple consisting of real image sample and its encoding or fake image sample and corresponding latent-space input to the generator.
In encoding-decoding model output is called as reconstruction.</p>
</section>
<section id="adversarial-autoencoders">
<h3><span class="section-number">1.5.5. </span>Adversarial Autoencoders<a class="headerlink" href="#adversarial-autoencoders" title="Permalink to this headline">#</a></h3>
<p>Autoencoders are composed from encoder and decoder. They learn nonlinear mappings in both directions.</p>
<p>There a couple of symptomps that GANs might suffer from:</p>
<ul class="simple">
<li><p>difficulties in getting the pair to converge</p></li>
<li><p>the generative model collapsing to generator very similar samples for different inputs</p></li>
<li><p>the discriminator loss converging quickly to zero so no reliable path for gradient updates to the generator</p></li>
</ul>
<p>However, there are several training tricks: batch normalization, to minimize the number of fully connected layers, leaky ReLU between intermediate layers rather than ReLU, feature matching, minibatch discrimination, heuristic averaging, one-sided label smoothing.
For image-to-image translation models are pix2pix and cyclegan.</p>
</section>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8253599">Paper</a></p>
</div>
</div>
</div>
<p>GANs provide a way to learn deep representations without extensively annotated data. They can be used in various tasks: image synthesis, semantic image editing, style transfer, image superresolution, and classfication. It is an emerging technique for both semisupervised and supervised learning. They achieve this by implicitly modeling high-dimensional distributions of the data. And can be used for various down-stream tasks such as semantic iamge editing, data augmentation, style transfer, image retrival etc. There are several GANs  architectures:</p>
</div>
</div>
</section>
<section id="image-to-image-translation-with-conditional-adversarial-networks">
<h2><span class="section-number">1.6. </span>Image-to-Image Translation with Conditional Adversarial Networks<a class="headerlink" href="#image-to-image-translation-with-conditional-adversarial-networks" title="Permalink to this headline">#</a></h2>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-1 sd-g-xs-1 sd-g-sm-1 sd-g-md-1 sd-g-lg-1 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
To Read</div>
<p class="sd-card-text"><a class="reference external" href="https://arxiv.org/pdf/1611.07004.pdf">Paper</a></p>
</div>
</div>
</div>
<p>In this paper, a general solution for image-to-image translation by using conditional adversarial networks is investigated and pix2pix model is proposed. Moreover, not only learning the mapping from input image to target image but also learning a loss function to train this mapping. This paper shows that there is no-need no longer for hand-engineer own mappings.</p>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nbs\papers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../../intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Welcome to my blog</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../art/art.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>rey’s Art</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Rabia Eda Yilmaz<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>